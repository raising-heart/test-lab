{
  "model": "llama2:latest",
  "temperature": 0.7,
  "max_tokens": 150,
  "messages": [
    {
      "role": "user",
      "content": "Hello"
    }
  ]
}